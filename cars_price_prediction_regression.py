# -*- coding: utf-8 -*-
"""cars-price-prediction-regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FnNps8UTKe8PkGKckggWpAIQjlDSNYZt
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
# %matplotlib inline
import seaborn as sns
# from google.colab import drive
from sklearn.decomposition import PCA
from sklearn.preprocessing import  OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.linear_model import LinearRegression

# link dataset
from google.colab import drive
# https://www.kaggle.com/datasets/hellbuoy/car-price-prediction/data
# https://raw.githubusercontent.com/xoghub/dicoding-MLT-Prediction-Analysis/main/CarPrice_Assignment.csv
drive.mount('/content/drive/')

url_dataset = 'https://raw.githubusercontent.com/xoghub/dicoding-MLT-Prediction-Analysis/main/CarPrice_Assignment.csv'
car = pd.read_csv(url_dataset)
car



"""Explanation about feature of dataset"""

car.drop(['car_ID', 'symboling', 'CarName'], inplace=True, axis=1)

car.info()

car.describe()

car.shape

"""Visualisasi Data dengan Boxplot untuk mengetahui apa ada tidaknya data yang diluar IQR."""

sns.boxplot(x=car['carheight'])

sns.boxplot(x=car['carlength'])

sns.boxplot(x=car['carwidth'])

sns.boxplot(x=car['wheelbase'])

sns.boxplot(x=car['horsepower'])

"""Kesimpulan ada beberapa data yang diluar IQR. Oleh karenanya, saya akan melakukan drop pada data yang diluar IQR."""

Q1 = car.quantile(0.25)
Q3 = car.quantile(0.75)
IQR = Q3-Q1

car = car[~((car<(Q1-1.5*IQR))|(car>(Q3+1.5*IQR))).any(axis=1)]
car.shape

"""*Analysis* Setiap Feature pada data dengan visualisasi sederhana menggunakan barchart."""

numerical_feature = ['wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight', 'enginesize', 'boreratio', 'stroke', 'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg', 'price']
categorical_feature = ['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem']

feature = categorical_feature[0]
count = car[feature].value_counts()
percent = 100*car[feature].value_counts(normalize=True)
df = pd.DataFrame({"jumlah sample": count, "persentase": percent.round(1)})
print(df)
count.plot(kind='bar', title=feature, )

"""Kesimpulan dari diatas hanya terdapat mobil dengan bahan bakar *gas* atau bensin."""

feature1 = categorical_feature[1]
count = car[feature1].value_counts()
percent = 100*car[feature1].value_counts(normalize=True)
df = pd.DataFrame({"jumlah sample": count, "persentase": percent.round(1)})
print(df)
count.plot(kind='bar', title=feature1)

"""Kesimpulan dari chart diatas, bahwasannya pada data ini, mobil didominasi oleh jenis *aspiration* dengan jenis std atau standar sebanyak 90.4% dibandingkan turbo yang hanya 9.6%"""

feature2 = categorical_feature[2]
count = car[feature2].value_counts()
percent = 100*car[feature2].value_counts(normalize=True)
df = pd.DataFrame({"jumlah sample": count, "persentase": percent.round(1)})
print(df)
count.plot(kind='bar', title=feature2)

"""Kesimpulan mobil pada data ini didominasi oleh 4 pintu sebanyak 54.7% dibandingkan 2 pintu sebanyak 45.3%"""

feature3 = categorical_feature[3]
count = car[feature3].value_counts()
percent = 100*car[feature3].value_counts(normalize=True)
df = pd.DataFrame({"jumlah sample": count, "persentase": percent.round(1)})
print(df)
count.plot(kind='bar', title=feature3)

"""Untuk feature carbody atau body mobil di dominasi oleh sedan(45%) dan hatchback (38%)."""

feature4 = categorical_feature[4]
count = car[feature4].value_counts()
percent = 100*car[feature4].value_counts(normalize=True)
df = pd.DataFrame({"jumlah sample": count, "persentase": percent.round(1)})
print(df)
count.plot(kind='bar', title=feature4)

feature5 = categorical_feature[5]
count = car[feature5].value_counts()
percent = 100*car[feature5].value_counts(normalize=True)
df = pd.DataFrame({"jumlah sample": count, "persentase": percent.round(1)})
print(df)
count.plot(kind='bar', title=feature5)

feature6 = categorical_feature[6]
count = car[feature6].value_counts()
percent = 100*car[feature6].value_counts(normalize=True)
df = pd.DataFrame({"jumlah sample": count, "persentase": percent.round(1)})
print(df)
count.plot(kind='bar', title=feature6)

feature7 = categorical_feature[7]
count = car[feature7].value_counts()
percent = 100*car[feature7].value_counts(normalize=True)
df = pd.DataFrame({"jumlah sample": count, "persentase": percent.round(1)})
print(df)
count.plot(kind='bar', title=feature7)

feature8 = categorical_feature[8]
count = car[feature8].value_counts()
percent = 100*car[feature8].value_counts(normalize=True)
df = pd.DataFrame({"jumlah sample": count, "persentase": percent.round(1)})
print(df)
count.plot(kind='bar', title=feature8)

car.hist(bins=50, figsize=(20,15))
plt.show()

cat_features = car.select_dtypes(include='object').columns.to_list()

for col in cat_features:
  sns.catplot(x=col, y="price", kind="bar", dodge=False, height = 4, aspect = 3,  data=car, palette="Set3")
  plt.title("Rata-rata 'price' Relatif terhadap - {}".format(col))
  plt.savefig("/content/drive/MyDrive/Data CVS/Rata-rata 'price' Relatif terhadap - {}.png".format(col))

"""Melakukan preprosessing feature one hot encoding untuk setiap categorical feature untuk mempermudah proses training."""

car = pd.concat([car, pd.get_dummies(car['fueltype'], prefix='fueltype')],axis=1)
car = pd.concat([car, pd.get_dummies(car['aspiration'], prefix='aspiration')],axis=1)
car = pd.concat([car, pd.get_dummies(car['doornumber'], prefix='doornumber')],axis=1)
car = pd.concat([car, pd.get_dummies(car['carbody'], prefix='carbody')],axis=1)
car = pd.concat([car, pd.get_dummies(car['drivewheel'], prefix='drivewheel')],axis=1)
car = pd.concat([car, pd.get_dummies(car['enginelocation'], prefix='enginelocation')],axis=1)
car = pd.concat([car, pd.get_dummies(car['enginetype'], prefix='enginetype')],axis=1)
car = pd.concat([car, pd.get_dummies(car['cylindernumber'], prefix='cylindernumber')],axis=1)
car = pd.concat([car, pd.get_dummies(car['fuelsystem'], prefix='fuelsystem')],axis=1)
car.drop(['fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem'], axis=1, inplace=True)
car.head()

# Mengamati hubungan antar fitur numerik dengan fungsi pairplot()
sns.pairplot(car, diag_kind = 'kde')

plt.figure(figsize=(10,8))
correlation_matrix = car.corr().round(2)

# Untuk menge-print nilai di dalam kotak, gunakan parameter anot=True
sns.heatmap(data=correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.3, )
plt.title("Correlation Matrix untuk Fitur Numerik ", size=20)
plt.savefig("/content/drive/MyDrive/Data CVS/Rata-rata 'price' Relatif terhadap - {}.png".format(col))

"""Berdasarkan plot diatas menunjukan bahwa tidak ada feature yang harus dilakukan dropping atau penghapusan feature dikarenakan semua feature memiliki correlation yang lebih dari 0.05 dengan feature price. Akan tetapi ada beberapa feature yang memiliki korelasi yang kuat antar satu sama lain, yaitu carlength, carwidth, dan cubweight yang memiliki korelasi 0.8. oleh karenanya kita akan melakukan dimensi reduksi pada feature tersebut. selain ke tiga feature tersebut terdapat citympg dengan highwaympg yang memiliki korelasi sebesar 0.98 dan kita akan melakukan hal yang sama untuk feature sebelumnya."""

sns.pairplot(car[['carlength', 'carwidth', 'carheight', 'curbweight']], plot_kws={"s": 3});

sns.pairplot(car[['citympg', 'highwaympg']], plot_kws={"s": 2});

"""Melakukan dimensi reduksi sebanyak 2 kali untuk dua kelompok feature:

1. ['carlength', 'carwidth', 'carheight', 'curbweight'] yang berhubungan dengan dimensi body mobil.
2. ['citympg', 'highwaympg'] yang berhubungan dengan kemampuan mobil untuk menempuh jarak per galon bahan bakar.
"""

pca = PCA(n_components=4, random_state=123)
pca.fit(car[['carlength', 'carwidth', 'carheight', 'curbweight']])
princ_comp = pca.transform(car[['carlength', 'carwidth', 'carheight', 'curbweight']])
pca.explained_variance_ratio_.round(3)

pca = PCA(n_components=1, random_state=123)
pca.fit(car[['carlength', 'carwidth', 'carheight', 'curbweight']])
princ_comp = pca.transform(car[['carlength', 'carwidth', 'carheight', 'curbweight']])
pca.explained_variance_ratio_.round(3)

car['overall_size'] =princ_comp
car.drop(['carlength', 'carwidth', 'carheight', 'curbweight'], axis=1, inplace=True)

pca = PCA(n_components=2, random_state=123)
pca.fit(car[['citympg', 'highwaympg']])
princ_comp = pca.transform(car[['citympg', 'highwaympg']])
pca.explained_variance_ratio_.round(3)

pca = PCA(n_components=1, random_state=123)
pca.fit(car[['citympg', 'highwaympg']])
princ_comp = pca.transform(car[['citympg', 'highwaympg']])
pca.explained_variance_ratio_.round(3)

car['mpg'] =princ_comp
car.drop(['citympg', 'highwaympg'], axis=1, inplace=True)

car

car.info()

"""Melakukan Splitting dataset menjadi 2 yaitu train dan test dataset dengan price sebagai target/variabel independent atau y serta feature lainnya sebagai variabel independent atau x ."""

X = car.drop(["price"],axis =1)
y = car["price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)

"""Melakukan pemeriksaan pada numerical feature dan melakukan normalisasi standar scaler serta melakukan pembulatan ke atas dengan mempertahankan 4 angka dibelakang koma."""

numerical_features = ['wheelbase', 'enginesize', 'boreratio', 'stroke', 'compressionratio', 'horsepower', 'peakrpm', 'mpg', 'overall_size',]
scaler = StandardScaler()
scaler.fit(X_train[numerical_features])
X_train[numerical_features] = scaler.transform(X_train.loc[:, numerical_features])
X_train[numerical_features].head()

X_train[numerical_features].describe().round(4)

"""Melakukan proses training ketiga model machine learning, yaitu linear regression, random forest regression, dan Ada Boosting algorithma untuk dapat memprediksi harga mobil yang sesuai dengan feature yang sudah dilakukan preprosessing terlebih dahulu."""

models = pd.DataFrame(index=['train_mse', 'test_mse'],
                      columns=['Linear Regression', 'RandomForest', 'Boosting'])

linear = LinearRegression()
linear.fit(X_train, y=y_train)

models.loc['train_mse','Linear Regression'] = mean_squared_error(y_pred = linear.predict(X_train), y_true=y_train)

RF = RandomForestRegressor(n_estimators=50, max_depth=16, random_state=23, n_jobs=-1)
RF.fit(X_train, y_train)

models.loc['train_mse','RandomForest'] = mean_squared_error(y_pred=RF.predict(X_train), y_true=y_train)

boosting = AdaBoostRegressor(learning_rate=0.05, random_state=55)
boosting.fit(X_train, y_train)
models.loc['train_mse','Boosting'] = mean_squared_error(y_pred=boosting.predict(X_train), y_true=y_train)

# Lakukan scaling terhadap fitur numerik pada X_test sehingga memiliki rata-rata=0 dan varians=1
X_test.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])

# Buat variabel mse yang isinya adalah dataframe nilai mse data train dan test pada masing-masing algoritma
mse = pd.DataFrame(columns=['train', 'test'], index=['Linear Regression','RandomForest','Boosting'])

# Buat dictionary untuk setiap algoritma yang digunakan
model_dict = {'Linear Regression': linear, 'RandomForest': RF, 'Boosting': boosting}

# Hitung Mean Squared Error masing-masing algoritma pada data train dan test
for name, model in model_dict.items():
    mse.loc[name, 'train'] = mean_squared_error(y_true=y_train, y_pred=model.predict(X_train))/1e3
    mse.loc[name, 'test'] = mean_squared_error(y_true=y_test, y_pred=model.predict(X_test))/1e3

# Panggil mse
mse

fig, ax = plt.subplots()
mse.sort_values(by='test', ascending=False).plot(kind='barh', ax=ax, zorder=3)
ax.grid(zorder=0)
plt.savefig("/content/drive/MyDrive/Data CVS/Evaluasi-Plot.png".format(fig))

"""Hasil Evaluasi menunjukan bahwa algoritma random forest regression memiliki loss yang yang rendah pada train dataset, akan tetapi pada test dataset masih lebih tinggi dibandingkan dengan algoritma boosting."""

prediksi = X_test.iloc[:1].copy()
pred_dict = {'y_true':y_test[:1]}
for name, model in model_dict.items():
    pred_dict['prediksi_'+name] = model.predict(prediksi).round(1)

pd.DataFrame(pred_dict)

"""Hasil akhir menunjukan bahwa prediksi Random Forest lebih baik dibandingkan prediksi Boosting atau linear regression."""